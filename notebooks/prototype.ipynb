{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698a1b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# ZELLE 1: SETUP & SPARK SESSION\n",
    "# ---------------------------------------------------------\n",
    "# Hier initialisieren wir den \"Driver\".\n",
    "# Wir nutzen Spark im \"Local Mode\" (master=\"local[*]\"), um einen\n",
    "# Cluster auf diesem Rechner zu simulieren.\n",
    "# Außerdem definieren wir relative Pfade, damit das Notebook bei jedem im Team läuft.\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, from_unixtime\n",
    "from pyspark.sql.types import DoubleType, LongType, StringType\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OpenSky Flight Data Analysis\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "RAW_PATH = os.path.join(current_dir, \"..\", \"data\", \"raw\")\n",
    "PROCESSED_PATH = os.path.join(current_dir, \"..\", \"data\", \"processed\")\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Lese Rohdaten aus: {RAW_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2ea843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# ZELLE 2: PHASE 1 - INGESTION (RAW ZONE / BRONZE LAYER)\n",
    "# ---------------------------------------------------------\n",
    "# Wir laden die Rohdaten (CSV) direkt aus dem \"Data Lake\" (lokaler Ordner).\n",
    "# Strategie: \"Schema-on-Read\" -> Spark versucht, Datentypen automatisch zu erkennen.\n",
    "# Dies entspricht dem Zugriff auf unstrukturierte Daten, bevor eine Validierung stattfindet.\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "df_raw = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(RAW_PATH)\n",
    "\n",
    "print(\"Schema der Rohdaten:\")\n",
    "df_raw.printSchema()\n",
    "\n",
    "print(f\"Anzahl der Zeilen im Raw Layer: {df_raw.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b445143d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# ZELLE 3: PHASE 2 - PROCESSING & STORAGE (SILVER LAYER)\n",
    "# ---------------------------------------------------------\n",
    "# Dies ist der Kern des Data Engineering (ETL):\n",
    "# 1. Cleaning: Casting von Strings zu korrekten Datentypen (Double/Long).\n",
    "# 2. Filter: Entfernen von Datensätzen ohne Geokoordinaten.\n",
    "# 3. Storage Optimization: Wir speichern die Daten als PARQUET.\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "df_cleaned = df_raw.select(\n",
    "    col(\"time\").cast(LongType()),\n",
    "    col(\"callsign\").cast(StringType()),\n",
    "    col(\"lat\").cast(DoubleType()),\n",
    "    col(\"lon\").cast(DoubleType()),\n",
    "    col(\"velocity\").cast(DoubleType()),\n",
    "    col(\"geoaltitude\").cast(DoubleType())\n",
    ")\n",
    "df_cleaned = df_cleaned.filter(col(\"lat\").isNotNull() & col(\"lon\").isNotNull())\n",
    "\n",
    "df_silver = df_cleaned.withColumn(\"flight_date\", to_date(from_unixtime(col(\"time\"))))\n",
    "\n",
    "print(f\"Schreibe optimierte Daten nach: {PROCESSED_PATH} ...\")\n",
    "df_silver.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"flight_date\") \\\n",
    "    .parquet(PROCESSED_PATH)\n",
    "\n",
    "print(\"ETL Erfolgreich. Daten sind jetzt im Silver Layer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef5fe0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# ZELLE 4: PHASE 3 - ANALYTICS (GOLD LAYER)\n",
    "# ---------------------------------------------------------\n",
    "# Ab hier arbeiten wir nur noch mit den optimierten Parquet-Daten (Silver Layer).\n",
    "# Wir führen eine OLAP-Aggregation durch (Group By + Count), um den\n",
    "# Datensatz mit den meisten Wegpunkten (aktivster Flug) zu finden.\n",
    "# Dank Parquet muss Spark hierfür nur die relevanten Spalten laden.\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "df_analytics = spark.read.parquet(PROCESSED_PATH)\n",
    "\n",
    "top_flights = df_analytics.groupBy(\"callsign\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc())\n",
    "\n",
    "print(\"Top 5 aktivste Callsigns:\")\n",
    "top_flights.show(5)\n",
    "\n",
    "target_callsign = top_flights.first()[\"callsign\"]\n",
    "print(f\"Wir visualisieren jetzt Flug: {target_callsign}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37ff01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# ZELLE 5: PHASE 4 - VISUALISIERUNG (SERVING LAYER)\n",
    "# ---------------------------------------------------------\n",
    "# Schnittstelle zwischen \"Big Data\" (Spark) und \"Small Data\" (Python/Pandas).\n",
    "# 1. Wir filtern auf EINEN spezifischen Flug (Reduktion der Datenmenge).\n",
    "# 2. Wir nutzen .toPandas(), um die Daten vom Cluster-Speicher in den lokalen RAM zu holen.\n",
    "# 3. Visualisierung der Route mittels Folium (interaktive Karte).\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "import folium\n",
    "import pandas as pd\n",
    "\n",
    "flight_trace = df_analytics \\\n",
    "    .filter(col(\"callsign\") == target_callsign) \\\n",
    "    .select(\"lat\", \"lon\", \"geoaltitude\", \"time\") \\\n",
    "    .orderBy(\"time\") \\\n",
    "    .limit(500) \\\n",
    "    .toPandas()\n",
    "\n",
    "if not flight_trace.empty:\n",
    "    center_lat = flight_trace[\"lat\"].mean()\n",
    "    center_lon = flight_trace[\"lon\"].mean()\n",
    "    \n",
    "    m = folium.Map(location=[center_lat, center_lon], zoom_start=6)\n",
    "\n",
    "    route_points = list(zip(flight_trace[\"lat\"], flight_trace[\"lon\"]))\n",
    "    \n",
    "    folium.PolyLine(route_points, color=\"red\", weight=3, opacity=0.8).add_to(m)\n",
    "    \n",
    "    folium.Marker(route_points[0], popup=\"Start\", icon=folium.Icon(color=\"green\")).add_to(m)\n",
    "    folium.Marker(route_points[-1], popup=\"Ende\", icon=folium.Icon(color=\"blue\")).add_to(m)\n",
    "    \n",
    "    display(m)\n",
    "else:\n",
    "    print(\"Keine Daten gefunden.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
